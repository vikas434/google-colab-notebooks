{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!gcloud auth application-default login\n"
   ],
   "metadata": {
    "id": "jtB_9ZGcxJSM"
   },
   "id": "jtB_9ZGcxJSM",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing Google Colab Enterprise File Upload and Vertex AI Call\n",
    "\n",
    "In this notebook, we'll walk through a process of uploading a file via Google Colab, processing the file content with Vertex AI, and saving the result to Google BigQuery.\n",
    "\n",
    "## Steps:\n",
    "1. Install necessary dependencies.\n",
    "2. Authenticate with Google Colab.\n",
    "3. Define and create the BigQuery table schema.\n",
    "4. Initialize Vertex AI.\n",
    "5. Load the pre-trained model from Vertex AI.\n",
    "6. Define the function to expand acronyms using the model.\n",
    "7. Upload the file.\n",
    "8. Process the file content.\n",
    "9. Save the result to BigQuery.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Step 1: Install Dependencies\n",
    "We'll need the `pandas-gbq` library to interact with BigQuery.\n"
   ],
   "metadata": {
    "id": "qlGkpy2cDTaa"
   },
   "id": "qlGkpy2cDTaa"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pandas-gbq\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJG0CSfzDW7i",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1697785765634,
     "user_tz": -330,
     "elapsed": 6,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "6323ec39-3243-4f70-bb5e-81d5afeff1b8"
   },
   "id": "gJG0CSfzDW7i",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Authenticate with Google Colab\n"
   ],
   "metadata": {
    "id": "fAF6EnoXDbDn"
   },
   "id": "fAF6EnoXDbDn"
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import auth as google_auth\n",
    "\n",
    "# Google Colab authentication\n",
    "google_auth.authenticate_user()\n"
   ],
   "metadata": {
    "id": "FReq7SoaDdYf"
   },
   "id": "FReq7SoaDdYf",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Define and Create BigQuery Table Schema\n"
   ],
   "metadata": {
    "id": "Yam-nfE-DgIn"
   },
   "id": "Yam-nfE-DgIn"
  },
  {
   "cell_type": "code",
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Define table schema\n",
    "client = bigquery.Client(project='<<project_id>>')\n",
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"Original Column Name\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"Expanded Column Name\", \"STRING\", mode=\"REQUIRED\")\n",
    "]\n",
    "\n",
    "# Create table if it doesn't exist\n",
    "table_id = f\"<<table_id>>\"\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "table = client.create_table(table, exists_ok=True)  # Creates table if it doesn't exist\n"
   ],
   "metadata": {
    "id": "IOdEiDuQDiIq"
   },
   "id": "IOdEiDuQDiIq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Initialize Vertex AI\n"
   ],
   "metadata": {
    "id": "DgchiAWwDoN7"
   },
   "id": "DgchiAWwDoN7"
  },
  {
   "cell_type": "code",
   "source": [
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "# Initialize Vertex AI\n",
    "try:\n",
    "    vertexai.init(project=\"<<project_id>>\", location=\"us-central1\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize Vertex AI: {e}\")\n",
    "    raise\n"
   ],
   "metadata": {
    "id": "4vkz5wstDqif"
   },
   "id": "4vkz5wstDqif",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Load the Pre-trained Model\n"
   ],
   "metadata": {
    "id": "K5uYPDGODvGP"
   },
   "id": "K5uYPDGODvGP"
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the pre-trained model\n",
    "try:\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load the pre-trained model: {e}\")\n",
    "    raise\n"
   ],
   "metadata": {
    "id": "r0pp2ci4Dwla"
   },
   "id": "r0pp2ci4Dwla",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6: Define Acronym Expander Function\n"
   ],
   "metadata": {
    "id": "QeP0zMFdDzCS"
   },
   "id": "QeP0zMFdDzCS"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define constants and model parameters\n",
    "INPUT_PROMPT_ACRONYM = \"\"\"\n",
    "Given an acronym, could you please expand the full meaning out of it?\n",
    "Here are a few glossaries that could help you expand:I am an AI trained to expand acronyms. Below is a glossary of some common acronyms and their expanded forms:\n",
    "\n",
    "- API stands for Application Programming Interface\n",
    "- HTTP stands for HyperText Transfer Protocol\n",
    "- AI stands for Artificial Intelligence\n",
    "- ML stands for Machine Learning\n",
    "- IoT stands for Internet of Things\n",
    "- SaaS stands for Software as a Service\n",
    "\n",
    "You can provide any acronym, and I'll try to expand it based on my training. Here are a couple of examples:\n",
    "\n",
    "Example 1:\n",
    "Input: AI\n",
    "Output: Artificial Intelligence\n",
    "\n",
    "Example 2:\n",
    "Input: IoT\n",
    "Output: Internet of Things\n",
    "\n",
    "Now, let's try with your input:\n",
    "Input: {}\n",
    "\"\"\"\n",
    "parameters = {\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 40\n",
    "}\n",
    "\n",
    "def acronym_expander(column_name):\n",
    "    print(f\"Expanding acronym for {column_name}\")\n",
    "    try:\n",
    "        response = model.predict(INPUT_PROMPT_ACRONYM.format(column_name), **parameters)\n",
    "        if \"output:\" in response.text:\n",
    "            expanded_name = response.text.split(\"output:\", 1)[1].strip()\n",
    "            # Prepare data for BigQuery\n",
    "            rows_to_insert = [{\n",
    "                \"Original Column Name\": column_name,\n",
    "                \"Expanded Column Name\": expanded_name\n",
    "            }]\n",
    "            # Insert data into BigQuery\n",
    "            errors = client.insert_rows_json(table_id, rows_to_insert)\n",
    "            if errors:\n",
    "                print(f\"Failed to insert rows: {errors}\")\n",
    "            else:\n",
    "                print(f\"Inserted {column_name}: {expanded_name} into {table_name}\")\n",
    "            return expanded_name\n",
    "        else:\n",
    "            print(f\"No output found for {column_name}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to expand acronym for {column_name}: {e}\")\n",
    "        return \"\"\n"
   ],
   "metadata": {
    "id": "SfaE5I7-DzsY"
   },
   "id": "SfaE5I7-DzsY",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: Upload the File\n"
   ],
   "metadata": {
    "id": "dKIbdWfsD6xG"
   },
   "id": "dKIbdWfsD6xG"
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload your CSV file:\")\n",
    "uploaded = files.upload()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "IVHzsQITD8nd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1697783569353,
     "user_tz": -330,
     "elapsed": 10512,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "e264e96a-5a2b-447c-e187-e859486bee24"
   },
   "id": "IVHzsQITD8nd",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 8: Process File Content\n"
   ],
   "metadata": {
    "id": "qnBQaCiEEDEL"
   },
   "id": "qnBQaCiEEDEL"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check if any file is uploaded\n",
    "if not uploaded:\n",
    "    print(\"No file uploaded. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "filename = list(uploaded.keys())[0]\n",
    "data = pd.read_csv(filename)\n",
    "\n",
    "# Create a directory for output files\n",
    "os.makedirs('/content/output_files', exist_ok=True)\n",
    "\n",
    "expanded_names = []\n",
    "for column_name in data.get('ColumnName', []):  # Use get to avoid KeyError\n",
    "    print(column_name)\n",
    "    expanded_name = acronym_expander(column_name)\n",
    "    expanded_names.append(expanded_name)\n",
    "\n",
    "output_data = pd.DataFrame({\n",
    "    'Original Column Name': data.get('ColumnName', []),\n",
    "    'Expanded Column Name': expanded_names\n",
    "})\n",
    "\n",
    "output_file_path = '/content/output_files/output.csv'\n",
    "output_data.to_csv(output_file_path, index=False)\n",
    "print(output_data)\n"
   ],
   "metadata": {
    "id": "GK1s92keEDuO"
   },
   "id": "GK1s92keEDuO",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import auth as google_auth\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Define table schema\n",
    "client = bigquery.Client(project='<<project_id>>')\n",
    "\n",
    "\n",
    "query_string = \"\"\"\n",
    "SELECT count(*) FROM `<<gbq_table_result>>`\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query_string)  # API request\n",
    "results = query_job.result()  # Waits for query to finish\n",
    "print(\"all good\")"
   ],
   "metadata": {
    "id": "RAFEzbf_3AqO"
   },
   "id": "RAFEzbf_3AqO",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, all the acronyms from the uploaded CSV file have been expanded using the Vertex AI model, and the results have been saved to BigQuery. Additionally, the results are saved to a CSV file in the `/content/output_files` directory.\n",
    "\n",
    "Feel free to download the result CSV file, or query the BigQuery table to see the expanded acronyms.\n"
   ],
   "metadata": {
    "id": "zuIF2iaqEJ5H"
   },
   "id": "zuIF2iaqEJ5H"
  }
 ]
}
